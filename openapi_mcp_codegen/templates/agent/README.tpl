# {{ mcp_name | capitalize }} Agent

This repository was auto-generated by *openapi-mcp-codegen*.

It wraps the **{{ mcp_name | capitalize }} MCP server** in a LangGraph
React agent and exposes it through an A2A server.
{% if a2a_proxy %}
## A2A Proxy mode (--with-a2a-proxy)
This agent was generated with --with-a2a-proxy. It runs a minimal WebSocket JSON-RPC upstream server only. To expose an A2A HTTP API, deploy the external â€œa2a-proxyâ€ first and point it at this agentâ€™s WebSocket endpoint.
- Upstream (this repo): ws://localhost:8000
- Proxy (separate project): translates A2A JSON-RPC/SSE to WS upstream

Quick start with a2a-proxy:
1) Install the proxy and create a config.yaml
2) Run this upstream: make run-with-proxy
3) Run the proxy: a2a-proxy -c config.yaml
4) Point A2A clients at the proxyâ€™s /a2a/v1
{% endif %}

See `Makefile` for useful targets.

## ðŸƒâ€â™‚ï¸ Getting Started

1.  Copy the environment template and fill in your credentials  
   ```bash
   cp .env.example .env
   $EDITOR .env
   ```

> NOTE  
> If the agent was generated with `--generate-system-prompt` (or any
> docstring-enhancement flag) you must export your LLM credentials, e.g.  
> `export OPENAI_API_KEY=â€¦`, `export ANTHROPIC_API_KEY=â€¦`.

{% if not a2a_proxy %}
2.  In one terminal, start the A2A server  
   ```bash
   make run-a2a
   ```

   - This server exposes:
     - AgentCard at /.well-known/agent.json describing capabilities/skills
     - A2A endpoints handled by DefaultRequestHandler and backed by InMemoryTaskStore
   - The executor streams status updates while tools run and emits a final artifact on completion.
{% else %}
2.  In one terminal, start the WS proxy  
   ```bash
   make run-with-proxy
   ```
3.  Start/deploy the external a2a-proxy and configure it to connect to ws://localhost:8000. The a2a-proxy will expose the A2A HTTP API to your clients.  
{% endif %}

{% if not a2a_proxy %}
3.  In another terminal, launch the interactive client  
   ```bash
   make run-a2a-client
   # (equivalent to: docker run -it --network=host ghcr.io/cnoe-io/agent-chat-cli:stable)
   ```
{% endif %}

Follow the on-screen prompts to chat with your {{ mcp_name | capitalize }} tools!

{% if generate_eval %}
## ðŸ“Š Evaluation

1.  Enter **evaluation mode** to create or extend the test set  
{% if not a2a_proxy %}
    ```bash
    make run-a2a-eval-mode   # same as: uv run python eval_mode.py
    ```
{% else %}
    ```bash
    # Evaluation mode relies on A2A; not available when generated with --with-a2a-proxy
    echo "Evaluation mode disabled (A2A not generated)."
    ```
{% endif %}
    â€¢ Test each tool, update / skip as needed  
    â€¢ Traces are stored in `eval/dataset.yaml`

2.  Execute the automated benchmark (uploads to LangSmith)  
    ```bash
    make eval               # runs eval/evaluate_agent.py
    ```

Ensure `LANGCHAIN_API_KEY`, `LANGCHAIN_ENDPOINT` (or equivalent) are set before running `make eval`.
{% endif %}

## ðŸ”Ž Observability with Langfuse

This agent is instrumented with the Langfuse v3 Python SDK (OTEL-based) for tracing LLM calls, tool usage, and agent steps.

Setup locally:

- Option A: Docker Compose (recommended)
  - Follow https://langfuse.com/docs/deployment/self-host to run Langfuse locally (default UI/API on http://localhost:3000)
- Option B: Cloud
  - Create a project on https://cloud.langfuse.com and obtain public/secret keys.

Configure the agent:

- Copy .env.example to .env
- Set:
  - LANGFUSE_PUBLIC_KEY=...
  - LANGFUSE_SECRET_KEY=...
  - LANGFUSE_HOST=http://localhost:3000  (or your cloud region URL)
  - LANGFUSE_TRACING_ENABLED=True
  - optionally LANGFUSE_DEBUG=True for verbose logs

Run the agent/A2A server as usual (make run-a2a). Visit Langfuse UI to see traces per request, including nested LLM/tool spans.

Notes:
- Ensure Langfuse platform version >= 3.63.0 for v3 SDK.
- Short-lived scripts call get_client().flush() automatically in some code paths; if you add your own scripts, call flush() or shutdown() before exit.
