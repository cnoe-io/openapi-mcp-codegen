# {{ mcp_name | capitalize }} Agent

This repository was auto-generated by *openapi-mcp-codegen*.

It wraps the **{{ mcp_name | capitalize }} MCP server** in a LangGraph
React agent and exposes it through an A2A server.

See `Makefile` for useful targets.

## 🏃‍♂️ Getting Started

1.  Copy the environment template and fill in your credentials  
   ```bash
   cp .env.example .env
   $EDITOR .env
   ```

> NOTE  
> If the agent was generated with `--generate-system-prompt` (or any
> docstring-enhancement flag) you must export your LLM credentials, e.g.  
> `export OPENAI_API_KEY=…`, `export ANTHROPIC_API_KEY=…`.

2.  In one terminal, start the A2A server  
   ```bash
   make run-a2a
   ```

3.  In another terminal, launch the interactive client  
   ```bash
   make run-a2a-client
   # (equivalent to: docker run -it --network=host ghcr.io/cnoe-io/agent-chat-cli:stable)
   ```

Follow the on-screen prompts to chat with your {{ mcp_name | capitalize }} tools!

{% if generate_eval %}
## 📊 Evaluation

1.  Enter **evaluation mode** to create or extend the test set  
    ```bash
    make run-a2a-eval-mode   # same as: uv run python eval_mode.py
    ```
    • Test each tool, update / skip as needed  
    • Traces are stored in `eval/dataset.yaml`

2.  Execute the automated benchmark (uploads to LangSmith)  
    ```bash
    make eval               # runs eval/evaluate_agent.py
    ```

Ensure `LANGCHAIN_API_KEY`, `LANGCHAIN_ENDPOINT` (or equivalent) are set before running `make eval`.
{% endif %}

## 🔎 Observability with Langfuse

This agent is instrumented with the Langfuse v3 Python SDK (OTEL-based) for tracing LLM calls, tool usage, and agent steps.

Setup locally:

- Option A: Docker Compose (recommended)
  - Follow https://langfuse.com/docs/deployment/self-host to run Langfuse locally (default UI/API on http://localhost:3000)
- Option B: Cloud
  - Create a project on https://cloud.langfuse.com and obtain public/secret keys.

Configure the agent:

- Copy .env.example to .env
- Set:
  - LANGFUSE_PUBLIC_KEY=...
  - LANGFUSE_SECRET_KEY=...
  - LANGFUSE_HOST=http://localhost:3000  (or your cloud region URL)
  - LANGFUSE_TRACING_ENABLED=True
  - optionally LANGFUSE_DEBUG=True for verbose logs

Run the agent/A2A server as usual (make run-a2a). Visit Langfuse UI to see traces per request, including nested LLM/tool spans.

Notes:
- Ensure Langfuse platform version >= 3.63.0 for v3 SDK.
- Short-lived scripts call get_client().flush() automatically in some code paths; if you add your own scripts, call flush() or shutdown() before exit.
