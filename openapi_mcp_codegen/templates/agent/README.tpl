# {{ mcp_name | capitalize }} Agent

This repository was auto-generated by *openapi-mcp-codegen*.

It wraps the **{{ mcp_name | capitalize }} MCP server** in a LangGraph
React agent and exposes it through an A2A server.
{% if a2a_proxy %}
## A2A Proxy mode (--with-a2a-proxy)
This agent was generated with --with-a2a-proxy. It runs a minimal WebSocket JSON-RPC upstream server only. To expose an A2A HTTP API, deploy the external â€œa2a-proxyâ€ first and point it at this agentâ€™s WebSocket endpoint.
- Upstream (this repo): ws://localhost:8000
- Proxy (separate project): translates A2A JSON-RPC/SSE to WS upstream

Quick start with a2a-proxy:
1) Install the proxy and create a config.yaml
2) Run this upstream: make run-with-proxy
3) Run the proxy: a2a-proxy -c config.yaml
4) Point A2A clients at the proxyâ€™s /a2a/v1
{% endif %}

See `Makefile` for useful targets.

## ðŸƒâ€â™‚ï¸ Getting Started

1.  Copy the environment template and fill in your credentials including [LLM values](https://cnoe-io.github.io/ai-platform-engineering/getting-started/docker-compose/configure-llms)
```bash
   cp .env.example .env
   $EDITOR .env
   ```

{% if not a2a_proxy %}
2.  In one terminal, start the A2A server
   ```bash
   make run-a2a
   ```
{% if enable_slim and not a2a_proxy %}
- To run HTTP A2A and SLIM bridge side-by-side using Docker Compose:
  ```bash
  make run-a2a-and-slim
  ```
  Environment:
  - SLIM_ENDPOINT=http://localhost:46357
  - A2A_PORT=8000 (HTTP A2A)
  - SLIM_PORT=8001 (container process for SLIM bridge)
{% endif %}

   - This server exposes:
     - AgentCard at /.well-known/agent.json describing capabilities/skills
     - A2A endpoints handled by DefaultRequestHandler and backed by InMemoryTaskStore
   - The executor streams status updates while tools run and emits a final artifact on completion.
{% else %}
2.  In one terminal, start the WS proxy
   ```bash
   make run-with-proxy
   ```
3.  Start/deploy the external a2a-proxy and configure it to connect to ws://localhost:${A2A_PORT:-8000}. The a2a-proxy will expose the A2A HTTP API to your clients.
{% endif %}

{% if not a2a_proxy %}
3.  In another terminal, launch the interactive client
   ```bash
   make run-a2a-client
   # (equivalent to: docker run -it --network=host ghcr.io/cnoe-io/agent-chat-cli:stable)
   ```
{% endif %}

Follow the on-screen prompts to chat with your {{ mcp_name | capitalize }} tools!

{% if generate_eval %}
## ðŸ“Š Evaluation

1.  Enter **evaluation mode** to create or extend the test set
{% if not a2a_proxy %}
    ```bash
    make run-a2a-eval-mode   # same as: uv run python eval_mode.py
    ```
{% else %}
    ```bash
    # Evaluation mode relies on A2A; not available when generated with --with-a2a-proxy
    echo "Evaluation mode disabled (A2A not generated)."
    ```
{% endif %}
    â€¢ Test each tool, update / skip as needed
    â€¢ Traces are stored in `eval/dataset.yaml`

2.  Execute the automated benchmark (uploads to LangFuse)
    ```bash
    make eval               # runs eval/evaluate_agent.py
    ```

Ensure `LANGCHAIN_API_KEY`, `LANGCHAIN_ENDPOINT` (or equivalent) are set before running `make eval`.
{% endif %}

## ðŸ”Ž Observability with Langfuse

This agent is instrumented with the Langfuse v3 Python SDK (OTEL-based) for tracing LLM calls, tool usage, and agent steps.

Setup locally:

- Option A: Docker Compose (recommended)
  - Follow https://langfuse.com/docs/deployment/self-host to run Langfuse locally (default UI/API on http://localhost:3000)
- Option B: Cloud
  - Create a project on https://cloud.langfuse.com and obtain public/secret keys.

Configure the agent:

- Copy .env.example to .env
- Set:
  - LANGFUSE_PUBLIC_KEY=...
  - LANGFUSE_SECRET_KEY=...
  - LANGFUSE_HOST=http://localhost:3000  (or your cloud region URL)
  - LANGFUSE_TRACING_ENABLED=True
  - optionally LANGFUSE_DEBUG=True for verbose logs

Run the agent/A2A server as usual (make run-a2a). Visit Langfuse UI to see traces per request, including nested LLM/tool spans.

## âš™ï¸ Environment Variables

### A2A Agent Configuration
| Variable | Default | Description |
|----------|---------|-------------|
| `A2A_HOST` | `0.0.0.0` | Host for the A2A agent server |
| `A2A_PORT` | `8000` | Port for the A2A agent server |

### MCP Server Configuration
| Variable | Default | Description |
|----------|---------|-------------|
| `MCP_HOST` | `localhost` | Host for local MCP server |
| `MCP_PORT` | `3000` | Port for local MCP server |

### Service-Specific Configuration
| Variable | Required | Description |
|----------|----------|-------------|
| `{{ mcp_name | upper }}_API_URL` | Yes | URL of the backend service |
| `{{ mcp_name | upper }}_TOKEN` | Yes | Authentication token for the service |
| `{{ mcp_name | upper }}_VERIFY_SSL` | No | Enable/disable SSL certificate verification (default: `true`). Falls back to `VERIFY_SSL` if not set. |
| `{{ mcp_name | upper }}_CA_BUNDLE` | No | Path to custom CA bundle for SSL verification |

### Usage Examples
```bash
# Run A2A agent on custom host/port
A2A_HOST=localhost A2A_PORT=9000 make run-a2a

# Run MCP server on custom host/port
MCP_HOST=0.0.0.0 MCP_PORT=4000 make run-mcp

# Disable SSL verification for self-signed certificates
{{ mcp_name | upper }}_VERIFY_SSL=false make run-a2a

# Use custom CA bundle for SSL verification
{{ mcp_name | upper }}_CA_BUNDLE=/path/to/ca-bundle.pem make run-a2a

# Set via environment file
echo "A2A_PORT=9000" >> .env
echo "MCP_PORT=4000" >> .env
echo "{{ mcp_name | upper }}_VERIFY_SSL=false" >> .env
```

Notes:
- Ensure Langfuse platform version >= 3.63.0 for v3 SDK.
- Short-lived scripts call get_client().flush() automatically in some code paths; if you add your own scripts, call flush() or shutdown() before exit.
