{% if file_headers %}
# {{ file_headers_copyright }}
# {{ file_headers_license }}
# {{ file_headers_message }}
{% endif %}
"""LangGraph React-agent wrapper for the generated MCP server."""

import asyncio
import importlib.util
import logging
import os
from pathlib import Path
from typing import Any, Dict
from dotenv import load_dotenv

from datetime import datetime
from langchain_core.tools import tool

from langchain_core.runnables import RunnableConfig
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_mcp_adapters.client import MultiServerMCPClient
from cnoe_agent_utils import LLMFactory
from langfuse import get_client

load_dotenv()          # makes values from .env available via os.getenv

logger = logging.getLogger(__name__)


# Default system prompt (auto-generated by the generator)
DEFAULT_SYSTEM_PROMPT = r"""{{ system_prompt }}"""

# Locate the generated MCP server module
spec = importlib.util.find_spec("mcp_{{ mcp_name }}.server")
if not spec or not spec.origin:
    raise ImportError("Cannot find mcp_{{ mcp_name }}.server module")
server_path = str(Path(spec.origin).resolve())

@tool
def get_current_time() -> str:
    """Return the current date-time in ISO-8601 format."""
    return datetime.now().isoformat()


async def create_agent(prompt: str | None = None, response_format=None):
    """
    Spin-up the MCP server as a subprocess via MultiServerMCPClient and build
    and returns the LangGraph agent **and its tool list**.
    """
    memory = MemorySaver()

    if prompt is None:
        prompt = DEFAULT_SYSTEM_PROMPT  # ‚Üê use literal string, no fallback

    api_url   = os.getenv("{{ mcp_name | upper }}_API_URL")
    api_token = os.getenv("{{ mcp_name | upper }}_TOKEN")
    if not api_url or not api_token:
        raise ValueError("Set {{ mcp_name | upper }}_API_URL and {{ mcp_name | upper }}_TOKEN env vars")

    client = MultiServerMCPClient(
        {
            "{{ mcp_name }}": {
                "command": "uv",
                "args": ["run", server_path],
                "env": {
                    "{{ mcp_name | upper }}_API_URL": api_url,
                    "{{ mcp_name | upper }}_TOKEN": api_token,
                },
                "transport": "stdio",
            },
            # Utility MCP
            "unix_timestamps_mcp": {
                "command": "npx",
                "args": ["-y", "github:Ivor/unix-timestamps-mcp"],
                "transport": "stdio",
            },
        }
    )
    mcp_tools = await client.get_tools()
    try:
        get_client().update_current_trace(tags=["{{ mcp_name }}-agent"])
    except Exception:
        pass
    tools = mcp_tools + [get_current_time]
    # Attach Langfuse callback handler so LangChain/LLM/tool calls are traced
    try:
        get_client().update_current_trace(tags=["{{ mcp_name }}-agent"])
    except Exception:
        pass
    agent = create_react_agent(
        LLMFactory().get_llm(),
        tools=tools,
        checkpointer=memory,
        prompt=prompt,
        response_format=response_format,
    )
    return agent, tools  # also return raw tool list for evaluators


# Convenience synchronous wrapper
def create_agent_sync(prompt: str | None = None, response_format=None):
    agent, tools = asyncio.run(create_agent(prompt, response_format))
    try:
        get_client().flush()
    except Exception:
        pass
    return agent, tools
