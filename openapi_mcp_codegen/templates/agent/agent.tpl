{% if file_headers %}
# {{ file_headers_copyright }}
# {{ file_headers_license }}
# {{ file_headers_message }}
{% endif %}
"""LangGraph React-agent wrapper for the generated MCP server."""

import asyncio
import logging
import os
import inspect
from pathlib import Path
from typing import Any, Dict
from langchain.tools import StructuredTool
from langchain_core.tools import BaseTool

from langchain_core.runnables import RunnableConfig
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_mcp_adapters.client import MultiServerMCPClient
from cnoe_agent_utils import LLMFactory
from langchain_core.messages import SystemMessage  # new

# Default system prompt (auto-generated by the generator)
DEFAULT_SYSTEM_PROMPT = r"""{{ system_prompt }}"""

logger = logging.getLogger(__name__)

# Absolute path to the generated MCP server’s main module
server_path = str((Path(__file__).parent / "{{ server_pkg }}" / "server.py").resolve())
if not Path(server_path).is_file():  # sanity-check
    raise FileNotFoundError(f"MCP server module not found at {server_path}")


async def create_agent(
    prompt: str | None = None,
    response_format=None,
    tools_subset: list[str] | None = None,
    tools: list[Any] | None = None,        # NEW – pass tools directly
):
    """
    Spin-up the MCP server as a subprocess via MultiServerMCPClient and build
    a LangGraph React agent that has access to its tools.

    Args:
        prompt (str | None): Optional prompt to use for the agent.
        response_format: Optional response format.
        tools_subset (list[str] | None): If provided, restrict the agent to only these tool names.

    Returns:
        The created LangGraph React agent.
    """
    memory = MemorySaver()

    # ---------------------------------------------------------------- prompt
    if prompt is None:
        prompt = DEFAULT_SYSTEM_PROMPT  # ← use literal string, no fallback

    api_url   = os.getenv("{{ mcp_name | upper }}_API_URL")
    api_token = os.getenv("{{ mcp_name | upper }}_TOKEN")
    if not api_url or not api_token:
        raise ValueError("Set {{ mcp_name | upper }}_API_URL and {{ mcp_name | upper }}_TOKEN env vars")

    # Determine whether we are running against a mock backend
    mock_api_flag = os.getenv("MOCK_API", "0").lower() not in {"0", "false", ""}

    client = MultiServerMCPClient(
        {
            "{{ mcp_name }}": {
                "command": "uv",
                "args": ["run", server_path],
                "env": (
                    lambda _base: (
                        _base
                        | (
                            {  # ← Azure creds only when mocking
                                "AZURE_OPENAI_DEPLOYMENT": os.getenv("AZURE_OPENAI_DEPLOYMENT", ""),
                                "AZURE_OPENAI_API_VERSION": os.getenv("AZURE_OPENAI_API_VERSION", ""),
                                "AZURE_OPENAI_ENDPOINT":    os.getenv("AZURE_OPENAI_ENDPOINT", ""),
                                "AZURE_OPENAI_API_KEY":     os.getenv("AZURE_OPENAI_API_KEY", ""),
                            }
                            if mock_api_flag
                            else {}
                        )
                    )
                )(
                    {
                        "{{ mcp_name | upper }}_API_URL": api_url,
                        "{{ mcp_name | upper }}_TOKEN":  api_token,
                        "MOCK_API": "1" if mock_api_flag else "0",
                    }
                ),
                "transport": "stdio",
            }
        }
    )

    if tools is not None:                     # ← local-tools path
        converted: list[Any] = []
        for t in tools:
            # Already a LangChain/BaseTool → use as-is
            if isinstance(t, BaseTool):
                converted.append(t)
                continue

            if inspect.iscoroutinefunction(t):
                # Build StructuredTool and ensure the coroutine branch is used
                st = StructuredTool.from_function(t)  # wraps signature
                st.coroutine = t                      # tell LC this is async
                converted.append(st)
            else:
                converted.append(StructuredTool.from_function(t))

        tools = converted
    else:                                     # ← original remote-MCP path
        tools = await client.get_tools()
    # Respect caller-supplied subset of tools -------------------------------
    if tools_subset:
        tools = [t for t in tools if getattr(t, "name", "") in tools_subset]
        if not tools:
            raise ValueError(
                f"No tools matched the requested subset {tools_subset!r}. "
                "Available tool names: "
                + ", ".join(getattr(t, 'name', '<unknown>') for t in await client.get_tools())
            )
    agent = create_react_agent(
        LLMFactory().get_llm(),
        tools=tools,
        checkpointer=memory,
        prompt=prompt,
        response_format=response_format,
    )
    return agent


# Convenience synchronous wrapper
def create_agent_sync(
    prompt: str | None = None,
    response_format=None,
    tools_subset: list[str] | None = None,
    tools: list[Any] | None = None,        # NEW
):
    return asyncio.run(create_agent(prompt, response_format, tools_subset, tools))




# (helper _generate_system_prompt removed)
