# Copyright 2025 CNOE
# SPDX-License-Identifier: Apache-2.0
# Generated by CNOE OpenAPI MCP Codegen tool

"""LangGraph React-agent wrapper for the generated MCP server."""

import asyncio
import importlib.util
import logging
import os
from pathlib import Path
from dotenv import load_dotenv

from datetime import datetime
from langchain_core.tools import tool

from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_mcp_adapters.client import MultiServerMCPClient
from cnoe_agent_utils import LLMFactory
from langfuse import get_client

load_dotenv()  # makes values from .env available via os.getenv

logger = logging.getLogger(__name__)


# Default system prompt (auto-generated by the generator)
DEFAULT_SYSTEM_PROMPT = r"""I am a pet store assistant capable of managing pets, orders, and user accounts. I can help you:

- Add, update, or delete pets in the store, ensuring no duplicate IDs.
- Find pets by status or tags, and retrieve pet details by ID.
- Upload pet images and manage inventory status.
- Place, retrieve, or delete orders, with specific ID requirements for valid responses.
- Create, update, or delete user accounts, and manage user login/logout.
- Provide user details based on username.

For any task, simply provide the necessary details, and I'll assist you efficiently."""

# Locate the generated MCP server module
spec = importlib.util.find_spec("mcp_petstore.server")
if not spec or not spec.origin:
    raise ImportError("Cannot find mcp_petstore.server module")
server_path = str(Path(spec.origin).resolve())


@tool
def get_current_time() -> str:
    """Return the current date-time in ISO-8601 format."""
    return datetime.now().isoformat()


async def create_agent(prompt: str | None = None, response_format=None):
    """
    Spin-up the MCP server as a subprocess via MultiServerMCPClient and build
    and returns the LangGraph agent **and its tool list**.
    """
    memory = MemorySaver()

    if prompt is None:
        prompt = DEFAULT_SYSTEM_PROMPT  # ‚Üê use literal string, no fallback

    api_url = os.getenv("PETSTORE_API_URL")
    api_token = os.getenv("PETSTORE_TOKEN")
    if not api_url or not api_token:
        raise ValueError("Set PETSTORE_API_URL and PETSTORE_TOKEN env vars")

    client = MultiServerMCPClient(
        {
            "petstore": {
                "command": "uv",
                "args": ["run", server_path],
                "env": {
                    "PETSTORE_API_URL": api_url,
                    "PETSTORE_TOKEN": api_token,
                },
                "transport": "stdio",
            },
            # Utility MCP
            "unix_timestamps_mcp": {
                "command": "npx",
                "args": ["-y", "github:Ivor/unix-timestamps-mcp"],
                "transport": "stdio",
            },
        }
    )
    mcp_tools = await client.get_tools()
    try:
        get_client().update_current_trace(tags=["petstore-agent"])
    except Exception:
        pass
    tools = mcp_tools + [get_current_time]
    # Attach Langfuse callback handler so LangChain/LLM/tool calls are traced
    try:
        get_client().update_current_trace(tags=["petstore-agent"])
    except Exception:
        pass
    agent = create_react_agent(
        LLMFactory().get_llm(),
        tools=tools,
        checkpointer=memory,
        prompt=prompt,
        response_format=response_format,
    )
    return agent, tools  # also return raw tool list for evaluators


# Convenience synchronous wrapper
def create_agent_sync(prompt: str | None = None, response_format=None):
    agent, tools = asyncio.run(create_agent(prompt, response_format))
    try:
        get_client().flush()
    except Exception:
        pass
    return agent, tools
