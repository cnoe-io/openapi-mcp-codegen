# Petstore Agent

This repository was auto-generated by *openapi-mcp-codegen*.

It wraps the **Petstore MCP server** in a LangGraph
React agent and exposes it through an A2A server.


See `Makefile` for useful targets.

## 🏃‍♂️ Getting Started

1.  Copy the environment template and fill in your credentials including [LLM values](https://cnoe-io.github.io/ai-platform-engineering/getting-started/docker-compose/configure-llms)
```bash
   cp .env.example .env
   $EDITOR .env
   ```


2.  In one terminal, start the A2A server  
   ```bash
   make run-a2a
   ```


   - This server exposes:
     - AgentCard at /.well-known/agent.json describing capabilities/skills
     - A2A endpoints handled by DefaultRequestHandler and backed by InMemoryTaskStore
   - The executor streams status updates while tools run and emits a final artifact on completion.



3.  In another terminal, launch the interactive client  
   ```bash
   make run-a2a-client
   # (equivalent to: docker run -it --network=host ghcr.io/cnoe-io/agent-chat-cli:stable)
   ```


Follow the on-screen prompts to chat with your Petstore tools!


## 📊 Evaluation

1.  Enter **evaluation mode** to create or extend the test set  

    ```bash
    make run-a2a-eval-mode   # same as: uv run python eval_mode.py
    ```

    • Test each tool, update / skip as needed  
    • Traces are stored in `eval/dataset.yaml`

2.  Execute the automated benchmark (uploads to LangFuse)
    ```bash
    make eval               # runs eval/evaluate_agent.py
    ```

Ensure `LANGCHAIN_API_KEY`, `LANGCHAIN_ENDPOINT` (or equivalent) are set before running `make eval`.


## 🔎 Observability with Langfuse

This agent is instrumented with the Langfuse v3 Python SDK (OTEL-based) for tracing LLM calls, tool usage, and agent steps.

Setup locally:

- Option A: Docker Compose (recommended)
  - Follow https://langfuse.com/docs/deployment/self-host to run Langfuse locally (default UI/API on http://localhost:3000)
- Option B: Cloud
  - Create a project on https://cloud.langfuse.com and obtain public/secret keys.

Configure the agent:

- Copy .env.example to .env
- Set:
  - LANGFUSE_PUBLIC_KEY=...
  - LANGFUSE_SECRET_KEY=...
  - LANGFUSE_HOST=http://localhost:3000  (or your cloud region URL)
  - LANGFUSE_TRACING_ENABLED=True
  - optionally LANGFUSE_DEBUG=True for verbose logs

Run the agent/A2A server as usual (make run-a2a). Visit Langfuse UI to see traces per request, including nested LLM/tool spans.

Notes:
- Ensure Langfuse platform version >= 3.63.0 for v3 SDK.
- Short-lived scripts call get_client().flush() automatically in some code paths; if you add your own scripts, call flush() or shutdown() before exit.